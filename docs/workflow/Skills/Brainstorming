 üéØ Hauptprobleme & Verbesserungsvorschl√§ge

  1. ‚ùå Verletzt eigene Superpowers-Regeln

  Problem: Der Skill hat eine Checklist, nutzt aber NICHT TodoWrite.

  Aus using-superpowers:
  "If a skill has a checklist, YOU MUST create TodoWrite todos for EACH item."

  Warum schlecht: User/Agent k√∂nnen Schritte vergessen, keine Fortschrittsverfolgung, widerspricht der eigenen Philosophie.

  Fix: Phase 0 hinzuf√ºgen: "Create TodoWrite todos for all 6 phases before starting."

  Anmerkung: Da stimme ich dir zu

  ---
  2. üö™ Unklare Exit-Bedingungen

  Problem: Wann ist jede Phase "fertig"? Skill sagt nicht explizit.

  Beispiel: Phase 1 sagt "Gather purpose, constraints, criteria" - aber woher wei√ü ich, dass ich genug Infos habe?

  Warum schlecht: Agent k√∂nnte zu fr√ºh zu Phase 2 springen oder endlos in Phase 1 Fragen stellen.

  Fix: Klare Acceptance Criteria f√ºr jede Phase:
  - Phase 1: "Can you answer: What problem? Why now? What defines success? If yes ‚Üí Phase 2"
  - Phase 2: "Have you proposed 2-3 approaches with trade-offs? If yes ‚Üí Phase 3"

  Anmerkung: Das ist ein typisches Problem. Wann ist es genug und wenn k√∂nnte es noch etwas mehr sein, wann sollte der Agent noch tiefer gehen? Das ist aber sehr schwer zu definieren. Man k√∂nnte Leitfragen schreiben, also die Dinge die wir auf jeden Fall wissen wollen aber welche Leitfragen zutreffen h√§ngt davonb ab was der user vor hat. Wenn der User eine ganze App plan ben√∂tigen wir andere Leitfragen als wenn der User ein Feature in einer bestehenden App anpassen oder erg√§nzen m√∂chte, daf√ºr ben√∂tigt der Agent auch mehr Kontext aus der bestehenden Codebase. Eigentlich denke ich auch das es unterschiedliche Skills sein sollten, greenfield/brownfield Man muss dann auch vorher kl√§ren was der User vor hat z.b. Neues Feature, Bug fix, Refactor, ganze App neu vom Rei√übrett planen usw. Man sollte f√ºr jeden Intent eigene kleine Skills entwickeln und einen Manager Skill davorschalten der erstmal abfragt was der User √ºberhaupt vor hat oder einfach den Prompt analysiert und selbst versteht was der User m√∂chte und dann den dazu entsprechenden Skill l√§dt.


  ---
  3. üìù Phase 4 (Documentation) zu vage

  Problem: Sagt "write design doc" aber gibt keine Struktur/Template.

  Aktuell: "Content: Capture the design as discussed and validated in Phase 3"

  Warum schlecht: Jeder schreibt Design Docs anders - keine Konsistenz, wichtige Sections fehlen.

  Fix: Konkretes Template:
  ## Problem Statement
  ## Success Criteria
  ## Proposed Solution
    - Architecture Overview
    - Key Components
    - Data Flow
  ## Alternative Approaches Considered
  ## Trade-offs & Risks
  ## Testing Strategy
  ## Out of Scope (YAGNI)

  Anmerkung: Ich stimme dir zu, wir br√§uchten ein Template damit diese Dokumente konsistent sind. 
  ---
  4. üîÑ Keine Scope-Creep-Prevention

  Problem: User f√ºgt immer mehr Features hinzu ("Oh und auch X, Y, Z...") - Skill hat keine Mechanismen dagegen.

  Warum schlecht: Brainstorming eskaliert, wird unm√∂glich zu implementieren, YAGNI wird ignoriert.

  Fix:
  - Phase 1: Explizite Frage "What is OUT of scope?"
  - Zwischen Phasen: "Scope Check" - hat sich Scope verdoppelt? ‚Üí Warnung
  - YAGNI-Checklist in Phase 3: "Ist jedes Feature wirklich f√ºr MVP n√∂tig?"

  Anmerkung: Ich denke nicht das wir ein Brainstorming einschr√§nken sollten denn das bremst die Kreativit√§t aus. Mann sollte grunds√§tzlich erst nach dem Brainstorming entscheiden was in den MVP muss und was erstmal drau√üen bleiben sollte und warum. Das ist ein eigener Prozess und sollte einen eigenen Skill bekommen.
  ---
  5. üîç Keine Best-Practices-Research

  Problem: Agent schl√§gt Approaches vor (Phase 2) ohne vorher Best Practices zu recherchieren.

  Beispiel: User will "Video Transcription API" - Agent schl√§gt Ans√§tze vor, ohne zu pr√ºfen: "Was machen andere? Welche Libraries gibt's? Was sagt aktuelle Dokumentation?"

  Warum schlecht: Veraltete/suboptimale Ans√§tze werden vorgeschlagen.

  Fix: Phase 1.5 einf√ºgen: "Research Best Practices"
  - Nutze REF MCP / WebSearch f√ºr aktuellen Stand der Technik
  - Dokumentiere Findings
  - Dann erst Phase 2 mit informierten Approaches

  Anmerkung: Da stimme ich dir absolut zu und deshalb habe ich in meinen Workflow genau das erg√§nzt aber wir sollten das in den Skill direkt mit einbacken. Wir k√∂nnten es auch so machen das ein Skill einen Plan schreibt was dann Version A darstelllt also der Rohdiamant und dann k√∂nte es einen 2. Skill geben der einen Research Auftrag hat der noch √ºber eine einfache Anfrage an Ref hinaus geht. Er k√∂nnte eine eine Liste Aufstellen mit einzelnen Aspekten unseres Planes, dinge die wir umsetzen m√ºssen, Probleme die wir optimal l√∂sen wollen und dann Ref und Perplexity Deep research nutzen um einen Bericht dar√ºber schreiben zu lassen wie man ein spezifisches Problem optimal und zeitgem√§√ü gel√∂st wird. welche Bibliothek warum perfekt daf√ºr w√§re. 

  ---
  6. ‚ö° Keine Komplexit√§ts-Anpassung

  Problem: Gleicher 6-Phasen-Prozess f√ºr "Add Button" (trivial) und "Build Auth System" (komplex).

  Warum schlecht: √úberkompliziert simple Tasks, untersch√§tzt komplexe Tasks.

  Fix: Skill-Start-Frage:
  "Is this a simple task (< 3 files, < 1 day) or complex task (> 5 files, > 2 days)?"
  - Simple ‚Üí Quick Mode (Phase 1+2+3 komprimiert)
  - Complex ‚Üí Deep Mode (voller 6-Phasen-Prozess)

  Anmerkung: da stimme ich dir zu 100% zu! Wir sollten den Ansatz an die individuelle Aufgabe angemessen anpassen und nicht one fits all machen.

  ---
  7. üîÅ Unklare Iterations-Guidance

  Problem: Was passiert, wenn User in Phase 3 sagt "Gut, aber √§nder diese 3 Dinge"?

  Aktuell: Skill sagt nur "you can go backward" aber nicht WANN oder WIE iteriert wird.

  Warum schlecht: Unklar ob "kleine √Ñnderung in Phase 3" oder "zur√ºck zu Phase 2" oder "zur√ºck zu Phase 1".

  Fix: Entscheidungsbaum:
  User Feedback in Phase 3:
  - "Change implementation detail" ‚Üí Iterate in Phase 3
  - "Different approach needed" ‚Üí Return to Phase 2
  - "Misunderstood requirement" ‚Üí Return to Phase 1
  - "Scope changed significantly" ‚Üí Return to Phase 1

  Anmerkung: Kannst du mir ein Beispiel geben damit ich besser verstehe was du meinst. Hast du einen L√∂sungsansatz (bitte kurz und knapp ohne Codebeispiele) 
  ---
  8. üéì Keine Learning-Capture bei Failed Approaches

  Problem: Wenn Approach in Phase 2 abgelehnt wird - warum? Was wurde gelernt?

  Warum schlecht: Gleiche schlechte Ideen werden sp√§ter wieder vorgeschlagen.

  Fix: Nach Phase 2 Rejection:
  - "Why was this approach rejected?" dokumentieren
  - In Design Doc Section "Rejected Approaches & Why"

  Anmerkung: wie w√ºrdest du das l√∂sen?

  ---
  9. üõë Phases 5-6 sollten optional sein

  Problem: Nicht jedes Brainstorming f√ºhrt zu Implementation - aber Skill geht direkt zu Worktree + Planning.

  Fix: Nach Phase 4 fragen:
  AskUserQuestion:
  "What's next?"
  Options:
    - "Implement now" ‚Üí Phase 5+6
    - "Save design for later" ‚Üí Stop here
    - "Need approval first" ‚Üí Stop here

    Anmerkung: ich stimme dir zu!

  ---
  10. üìä Keine Validierung der Design-Qualit√§t

  Problem: Phase 3 endet mit User "Ja, sieht gut aus" - aber wurde wirklich alles bedacht?

  Fix: Design-Quality-Checklist vor Phase 4:
  Pre-Documentation Checklist:
  - [ ] Error handling addressed?
  - [ ] Testing strategy defined?
  - [ ] Performance considerations noted?
  - [ ] Security implications discussed?
  - [ ] Rollback/Migration plan (if needed)?

    Anmerkung: ich stimme dir zu!

  ---
  üéØ Zusammenfassung: Top 3 Kritische Verbesserungen

  | #   | Problem                      | Impact                                   | Fix                           |
  |-----|------------------------------|------------------------------------------|-------------------------------|
  | 1   | Keine TodoWrite-Integration  | Schritte werden vergessen, kein Tracking | TodoWrite f√ºr alle 6 Phasen   |
  | 2   | Unklare Exit-Conditions      | Zu fr√ºh/sp√§t weitergehen                 | Acceptance Criteria pro Phase |
  | 3   | Kein Best-Practices-Research | Veraltete/suboptimale Approaches         | Phase 1.5: REF MCP Research   |

  ---

---
---

 ---
  üìç Antworten auf deine Fragen

  Punkt 7: Iterations-Guidance - Beispiel & L√∂sung

  Beispiel-Szenario:
  User: "Ich will ein Video-Upload-Feature"
  Agent: [Phase 1-2 abgeschlossen]
  Agent: [Phase 3] "Design: Videos werden direkt zu S3 hochgeladen,
         Lambda-Funktion erstellt Thumbnails, DB speichert Metadaten"

  User: "Gut, aber ich will kein S3, sondern lokales Storage"

  Problem aktuell: Skill sagt nicht, was der Agent tun soll.

  L√∂sung (kurz):
  Agent analysiert Feedback-Impact:
  - "Implementation-Detail ge√§ndert" (S3 ‚Üí Local Storage, aber Architektur bleibt) ‚Üí Iterate in Phase 3 (nur betroffene Section neu schreiben)
  - "Architektur fundamental anders" (z.B. "Nicht asynchron, sondern synchron") ‚Üí Return to Phase 2 (neue Approaches n√∂tig)

  Wie im Skill umsetzen:
  Einfache Regel nach User-Feedback:
  "Does this change affect:
  - Just implementation? ‚Üí Edit current section
  - Architecture approach? ‚Üí Return to Phase 2
  - Core requirements? ‚Üí Return to Phase 1"

  ---
  Punkt 8: Learning-Capture - Wie l√∂sen?

  Mein Ansatz:

  W√§hrend Phase 2 (Approach Selection):
  Agent schl√§gt 3 Approaches vor via AskUserQuestion
  User w√§hlt Approach B

  Agent fragt EXPLIZIT:
  "Why not Approach A or C? Understanding your reasoning helps me
   learn for future designs."

  User: "Approach A ist zu komplex f√ºr MVP, C hat Performance-Risiko"

  Agent dokumentiert:
  ## Rejected Approaches
  ### Approach A: Event-driven with Message Queue
  **Rejected because:** Too complex for MVP, team has no RabbitMQ experience
  **Context:** Small team, 3-month timeline, prefer simple solutions

  ### Approach C: Direct DB polling
  **Rejected because:** Performance risk with high video volume
  **Context:** Expecting 10k+ videos/month

  Warum das funktioniert:
  - Explizite Frage zwingt User, Reasoning zu artikulieren
  - Dokumentation hilft sp√§terer Review ("Warum haben wir das damals nicht gemacht?")
  - N√§chstes Brainstorming kann auf Learnings aufbauen

  Im Skill:
  Nach Phase 2 Selection:
  - User selects approach via AskUserQuestion
  - Agent asks: "What made you choose X over Y/Z?"
  - Document rejected approaches with reasoning in design doc

  ---
  üéØ Deine brillanten Erg√§nzungen

  1. Intent-basierter Manager-Skill (Punkt 2)

  Dein Vorschlag ist genial! Das l√∂st nicht nur Exit-Conditions, sondern auch Komplexit√§ts-Anpassung (Punkt 6).

  M√∂gliche Struktur:
  brainstorming-manager (Router-Skill)
  ‚îú‚îÄ Analyzes user intent
  ‚îú‚îÄ Asks clarifying question if unclear
  ‚îî‚îÄ Dispatches to:
     ‚îú‚îÄ brainstorming:greenfield (neue App von Scratch)
     ‚îú‚îÄ brainstorming:feature (Feature in bestehender App)
     ‚îú‚îÄ brainstorming:refactor (Code-Verbesserung)
     ‚îî‚îÄ brainstorming:bugfix (Bug-Analyse & Fix-Design)

  Jeder Sub-Skill hat spezialisierte Leitfragen:
  - Greenfield: Tech Stack? Team Size? Timeline?
  - Feature: Existing Architecture? Integration Points? Migration needed?
  - Refactor: What's broken? Performance/Maintainability goal?

  ---
  2. Research-Skill als separater Schritt (Punkt 5)

  Deine 2-Stufen-Idee ist hervorragend:

  Phase 1-2: Brainstorming ‚Üí Version A (Rohdiamant)
  ‚Üì
  Phase X: Deep Research Skill
    - Analysiert Plan
    - Identifiziert Key Technical Decisions
    - F√ºr jede Decision: REF + Perplexity Research
    - Erstellt Research Report mit Empfehlungen
  ‚Üì
  Phase 3: Informed Design (Version B - poliert)

  Beispiel Technical Decisions:
  - "Video Transcoding" ‚Üí Research: FFmpeg vs Cloud (AWS MediaConvert)
  - "Authentication" ‚Üí Research: JWT vs Session-based, OAuth2 Libraries
  - "State Management" ‚Üí Research: Zustand vs Redux vs Jotai

  Jede Decision bekommt mini-report mit aktuellen Best Practices.

  ---
  3. Scope-Creep = Kreativit√§t, MVP-Selektion = eigener Skill (Punkt 4)

  Du hast absolut recht! Brainstorming soll explorativ sein.

  Besserer Ansatz:
  Brainstorming-Phase: Alles sammeln (no limits!)
  ‚Üì
  MVP-Prioritization-Skill (separater Skill):
    - Listet alle Features aus Design Doc
    - Kategorisiert: Must-Have / Nice-to-Have / Future
    - AskUserQuestion f√ºr jedes Feature: "MVP or Later?"
    - Erstellt MVP-Scope-Dokument
  ‚Üì
  Implementation nur f√ºr MVP-Scope

  Vorteil:
  - Brainstorming kann "Big Picture" denken
  - MVP-Selektion ist bewusster, dokumentierter Prozess
  - Nicht-MVP-Features gehen nicht verloren, sondern in Backlog

  ---
  üöÄ Wie weitermachen?

  Du hast jetzt sehr klare Ideen, wie ein besserer Brainstorming-Workflow aussehen k√∂nnte. M√∂chtest du, dass wir:

  Option A: Einen komplett neuen, verbesserten Brainstorming-Skill entwickeln mit:
  - Intent-Manager (Router)
  - Spezialisierten Sub-Skills (Greenfield/Feature/Refactor/Bug)
  - Research-Phase mit REF/Perplexity
  - MVP-Prioritization als separater Follow-up-Skill

  Option B: Schrittweise vorgehen:
  1. Erst Intent-Manager-Skill entwickeln
  2. Dann einen Sub-Skill (z.B. brainstorming:feature) als Proof-of-Concept
  3. Research-Skill
  4. MVP-Prioritization-Skill